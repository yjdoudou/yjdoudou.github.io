<!-- build time:Fri Apr 29 2022 10:31:46 GMT+0800 (GMT+08:00) --><!DOCTYPE html><html class="theme-next gemini use-motion" lang="zh"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta name="theme-color" content="#222"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css"><link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css"><link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4"><link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222"><meta name="keywords" content="hadoop，大数据,"><meta name="description" content="一、大数据概论1.1.大数据概念大数据（BigData）：指无法在一定时间范围内用常规软件工具进行捕捉、管理和处理的数据集合，是需要新处理模式才能具有更强的决策力、洞察发现力和流程优化能力的海量、高增长率和多样化的信息资产。1234按顺序给出数据存储单位：bit、Byte、KB、MB、GB、TB、PB、EB、ZB、YB、BB、NB、DB。1Byte &#x3D; 8bit 1K &#x3D; 10"><meta property="og:type" content="article"><meta property="og:title" content="hadoop安装"><meta property="og:url" content="https://yjdoudou.github.io/2022/04/11/21104/index.html"><meta property="og:site_name" content="流水的博客"><meta property="og:description" content="一、大数据概论1.1.大数据概念大数据（BigData）：指无法在一定时间范围内用常规软件工具进行捕捉、管理和处理的数据集合，是需要新处理模式才能具有更强的决策力、洞察发现力和流程优化能力的海量、高增长率和多样化的信息资产。1234按顺序给出数据存储单位：bit、Byte、KB、MB、GB、TB、PB、EB、ZB、YB、BB、NB、DB。1Byte &#x3D; 8bit 1K &#x3D; 10"><meta property="og:locale" content="zh"><meta property="og:image" content="https://yjdoudou.github.io/2022/04/11/21104/1.png"><meta property="og:image" content="https://yjdoudou.github.io/2022/04/11/21104/2.png"><meta property="og:image" content="https://yjdoudou.github.io/2022/04/11/21104/3.png"><meta property="og:image" content="https://yjdoudou.github.io/2022/04/11/21104/4.png"><meta property="og:image" content="https://yjdoudou.github.io/2022/04/11/21104/5.png"><meta property="og:image" content="https://yjdoudou.github.io/2022/04/11/21104/6.png"><meta property="og:image" content="https://yjdoudou.github.io/2022/04/11/21104/7.png"><meta property="og:image" content="https://yjdoudou.github.io/2022/04/11/21104/8.png"><meta property="og:image" content="https://yjdoudou.github.io/2022/04/11/21104/9.png"><meta property="og:image" content="https://yjdoudou.github.io/2022/04/11/21104/10.png"><meta property="og:image" content="https://yjdoudou.github.io/2022/04/11/21104/11.png"><meta property="og:image" content="https://yjdoudou.github.io/2022/04/11/21104/12.png"><meta property="og:image" content="https://yjdoudou.github.io/2022/04/11/21104/13.png"><meta property="og:image" content="https://yjdoudou.github.io/2022/04/11/21104/14.png"><meta property="og:image" content="https://yjdoudou.github.io/2022/04/11/21104/15.png"><meta property="og:image" content="https://yjdoudou.github.io/2022/04/11/21104/16.png"><meta property="og:image" content="https://yjdoudou.github.io/2022/04/11/21104/17.png"><meta property="og:image" content="https://yjdoudou.github.io/2022/04/11/21104/18.png"><meta property="og:image" content="https://yjdoudou.github.io/2022/04/11/21104/19.png"><meta property="og:image" content="https://yjdoudou.github.io/2022/04/11/21104/20.png"><meta property="og:image" content="https://yjdoudou.github.io/2022/04/11/21104/21.png"><meta property="og:image" content="https://yjdoudou.github.io/2022/04/11/21104/22.png"><meta property="og:image" content="https://yjdoudou.github.io/2022/04/11/21104/23.png"><meta property="og:image" content="https://yjdoudou.github.io/2022/04/11/21104/24.png"><meta property="og:image" content="https://yjdoudou.github.io/2022/04/11/21104/25.png"><meta property="og:image" content="https://yjdoudou.github.io/2022/04/11/21104/26.png"><meta property="og:image" content="https://yjdoudou.github.io/2022/04/11/21104/27.png"><meta property="og:image" content="https://yjdoudou.github.io/2022/04/11/21104/28.png"><meta property="og:image" content="https://yjdoudou.github.io/2022/04/11/21104/29.png"><meta property="og:image" content="https://yjdoudou.github.io/2022/04/11/21104/30.png"><meta property="og:image" content="https://yjdoudou.github.io/2022/04/11/21104/31.png"><meta property="og:image" content="https://yjdoudou.github.io/2022/04/11/21104/32.png"><meta property="og:image" content="https://yjdoudou.github.io/2022/04/11/21104/33.png"><meta property="og:image" content="https://yjdoudou.github.io/2022/04/11/21104/34.png"><meta property="og:image" content="https://yjdoudou.github.io/2022/04/11/21104/35.png"><meta property="og:image" content="https://yjdoudou.github.io/2022/04/11/21104/36.png"><meta property="og:image" content="https://yjdoudou.github.io/2022/04/11/21104/37.png"><meta property="og:image" content="https://yjdoudou.github.io/2022/04/11/21104/38.png"><meta property="og:image" content="https://yjdoudou.github.io/2022/04/11/21104/39.png"><meta property="og:image" content="https://yjdoudou.github.io/2022/04/11/21104/40.png"><meta property="og:image" content="https://yjdoudou.github.io/2022/04/11/21104/41.png"><meta property="og:image" content="https://yjdoudou.github.io/2022/04/11/21104/42.png"><meta property="og:image" content="https://yjdoudou.github.io/2022/04/11/21104/43.png"><meta property="og:image" content="https://yjdoudou.github.io/2022/04/11/21104/44.png"><meta property="og:image" content="https://yjdoudou.github.io/2022/04/11/21104/45.png"><meta property="og:image" content="https://yjdoudou.github.io/2022/04/11/21104/46.png"><meta property="og:image" content="https://yjdoudou.github.io/2022/04/11/21104/47.png"><meta property="og:image" content="https://yjdoudou.github.io/2022/04/11/21104/48.png"><meta property="og:image" content="https://yjdoudou.github.io/2022/04/11/21104/49.png"><meta property="og:image" content="https://yjdoudou.github.io/2022/04/11/21104/50.png"><meta property="og:image" content="https://yjdoudou.github.io/2022/04/11/21104/51.png"><meta property="og:image" content="https://yjdoudou.github.io/2022/04/11/21104/52.png"><meta property="og:image" content="https://yjdoudou.github.io/2022/04/11/21104/53.png"><meta property="og:image" content="https://yjdoudou.github.io/2022/04/11/21104/54.png"><meta property="og:image" content="https://yjdoudou.github.io/2022/04/11/21104/55.png"><meta property="og:image" content="https://yjdoudou.github.io/2022/04/11/21104/56.png"><meta property="og:image" content="https://yjdoudou.github.io/2022/04/11/21104/57.png"><meta property="og:image" content="https://yjdoudou.github.io/2022/04/11/21104/58.png"><meta property="og:image" content="https://yjdoudou.github.io/2022/04/11/21104/59.png"><meta property="og:image" content="https://yjdoudou.github.io/2022/04/11/21104/60.png"><meta property="og:image" content="https://yjdoudou.github.io/2022/04/11/21104/61.png"><meta property="og:image" content="https://yjdoudou.github.io/2022/04/11/21104/62.png"><meta property="og:image" content="https://yjdoudou.github.io/2022/04/11/21104/63.png"><meta property="og:image" content="https://yjdoudou.github.io/2022/04/11/21104/64.png"><meta property="og:image" content="https://yjdoudou.github.io/2022/04/11/21104/65.png"><meta property="og:image" content="https://yjdoudou.github.io/2022/04/11/21104/66.png"><meta property="og:image" content="https://yjdoudou.github.io/2022/04/11/21104/67.png"><meta property="og:image" content="https://yjdoudou.github.io/2022/04/11/21104/68.png"><meta property="og:image" content="https://yjdoudou.github.io/2022/04/11/21104/69.png"><meta property="og:image" content="https://yjdoudou.github.io/2022/04/11/21104/70.png"><meta property="article:published_time" content="2022-04-11T03:10:01.000Z"><meta property="article:modified_time" content="2022-04-29T02:25:36.716Z"><meta property="article:author" content="流水"><meta property="article:tag" content="hadoop，大数据"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://yjdoudou.github.io/2022/04/11/21104/1.png"><script type="text/javascript" id="hexo.configurations">var NexT=window.NexT||{},CONFIG={root:"/",scheme:"Gemini",version:"5.1.4",sidebar:{position:"left",display:"post",offset:12,b2t:!0,scrollpercent:!0,onmobile:!1},fancybox:!0,tabs:!0,motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},duoshuo:{userId:"0",author:"Author"},algolia:{applicationID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}}}</script><link rel="canonical" href="https://yjdoudou.github.io/2022/04/11/21104/"><title>hadoop安装 | 流水的博客</title><meta name="generator" content="Hexo 4.2.1"></head><body itemscope itemtype="http://schema.org/WebPage" lang="zh"><div class="container sidebar-position-left page-post-detail"><div class="headband"></div><header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-wrapper"><div class="site-meta"><div class="custom-logo-site-title"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">流水的博客</span> <span class="logo-line-after"><i></i></span></a></div><p class="site-subtitle">知行合一</p></div><div class="site-nav-toggle"><button><span class="btn-bar"></span> <span class="btn-bar"></span> <span class="btn-bar"></span></button></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-首页"><a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i><br>首页</a></li><li class="menu-item menu-item-归档"><a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i><br>归档</a></li><li class="menu-item menu-item-分类"><a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i><br>分类</a></li><li class="menu-item menu-item-标签"><a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i><br>标签</a></li><li class="menu-item menu-item-关于"><a href="/about/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i><br>关于</a></li><li class="menu-item menu-item-search"><a href="javascript:;" class="popup-trigger"><i class="menu-item-icon fa fa-search fa-fw"></i><br>Search</a></li></ul><div class="site-search"><div class="popup search-popup local-search-popup"><div class="local-search-header clearfix"><span class="search-icon"><i class="fa fa-search"></i> </span><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span><div class="local-search-input-wrapper"><input autocomplete="off" placeholder="Searching..." spellcheck="false" type="text" id="local-search-input"></div></div><div id="local-search-result"></div></div></div></nav></div></header><main id="main" class="main"><div class="main-inner"><div class="content-wrap"><div id="content" class="content"><div id="posts" class="posts-expand"><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><div class="post-block"><link itemprop="mainEntityOfPage" href="https://yjdoudou.github.io/2022/04/11/21104/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="流水"><meta itemprop="description" content=""><meta itemprop="image" content="/images/head.jpg"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="流水的博客"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">hadoop安装</h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">Posted on</span> <time title="Post created" itemprop="dateCreated datePublished" datetime="2022-04-11T11:10:01+08:00">2022-04-11 </time><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-calendar-check-o"></i> </span><span class="post-meta-item-text">Post modified&#58;</span> <time title="Post modified" itemprop="dateModified" datetime="2022-04-29T10:25:36+08:00">2022-04-29 </time></span><span class="post-category"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-folder-o"></i> </span><span class="post-meta-item-text">In</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/" itemprop="url" rel="index"><span itemprop="name">大数据</span> </a></span></span><span class="post-comments-count"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><a href="/2022/04/11/21104/#comments" itemprop="discussionUrl"><span class="post-comments-count valine-comment-count" data-xid="/2022/04/11/21104/" itemprop="commentCount"></span></a></span></div></header><div class="post-body" itemprop="articleBody"><h1 id="一、大数据概论"><a href="#一、大数据概论" class="headerlink" title="一、大数据概论"></a>一、大数据概论</h1><p><strong>1.1.大数据概念</strong><br>大数据（BigData）：指无法在一定时间范围内用常规软件工具进行捕捉、管理和处理的数据集合，是需要新处理模式才能具有更强的决策力、洞察发现力和流程优化能力的海量、高增长率和多样化的信息资产。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">按顺序给出数据存储单位：</span><br><span class="line">bit、Byte、KB、MB、GB、TB、PB、EB、ZB、YB、BB、NB、DB。</span><br><span class="line">1Byte &#x3D; 8bit 1K &#x3D; 1024Byte 1MB &#x3D; 1024K</span><br><span class="line">1G &#x3D; 1024M 1T &#x3D; 1024G 1P&#x3D; 1024T</span><br></pre></td></tr></table></figure><p>大数据主要解决，海量数据的采集、存储和分析计算问题。</p><a id="more"></a><p><strong>1.2.大数据特点（4V）</strong><br><strong>（1）Volume（大量）</strong></p><blockquote><p>截至目前，人类生产的所有印刷材料的数据量是200PB，而历史上全人类总共说过的话的数据量大约是5EB。当前，典型个人计算机硬盘的容量为TB量级，而一些大企业的数据量已经接近EB量级。</p></blockquote><p><strong>（2）Velocity（高速）</strong></p><blockquote><p>这是大数据区分于传统数据挖掘的最显著特征。根据IDC的”数字宇宙”的报<br>告，预计到2025年，全球数据使用量将达到163ZB。在如此海量的数据面前，处理数据的效率就是企业的生命。</p></blockquote><p><strong>（3）Variety（多样）</strong></p><blockquote><p>这种类型的多样性也让数据被分为结构化数据和非结构化数据。相对于以往便于存储的以数据库/文本为主的结构化数据，非结构化数据越来越多，包括网络日志、音频、视频、图<br>片、地理位置信息等，这些多类型的数据对数据的处理能力提出了更高要求。</p></blockquote><p><strong>（4）Value（低价值密度）</strong></p><blockquote><p>价值密度的高低与数据总量的大小成反比。如何快速对有价值数据”提纯”成为目前大数据背景下待解决的难题。</p></blockquote><p><strong>1.3.大数据应用场景</strong><br>（1）抖音：推荐的都是你喜欢的视频<br>（2）电商站内广告推荐：给用户推荐可能喜欢的商品<br>（3）零售：分析用户消费习惯，为用户购买商品提供方便，从而提升商品销量。经典案例，纸尿布+啤酒。<br><img src="/2022/04/11/21104/1.png" alt></p><p>（4）物流仓储：京东物流，上午下单下午送达、下午下单次日上午送达<br>（5）保险：海量数据挖掘及风险预测，助力保险行业精准营销，提升精细化定价能力。<br>（6）金融：多维度体现用户特征，帮助金融机构推荐优质客户，防范欺诈风险。<br>（7）房产：大数据全面助力房地产行业，打造精准投策与营销，选出更合适的地，建造更合适的楼，卖给更合适的人。<br>（8）人工智能 + 5G + 物联网 + 虚拟与现实<br><img src="/2022/04/11/21104/2.png" alt="在这里插入图片描述"><br><strong>1.4.大数据发展前景</strong><br>（1）党的十九大提出”推动互联网、大数据、人工智能和实体经济深度融合”。<br>（2）2020年初，中央推出34万亿”新基建”投资计划<br><img src="/2022/04/11/21104/3.png" alt="在这里插入图片描述"><br>（3）下一个风口<br>2020年是5G的元年，国家在大力铺设5G设备，2021年就是5G手机应用的开始，也是大数据要爆发的1年。5G带来的是每秒钟10g的数据，会给每家公司都带来海量的数据。那么传统的Java工具根本解决不了海量数据的存储。就更不用说海量数据的计算了。如果你对5G的感触不够深，可以回忆一下3G和4G的区别。3G时只能打电话、发短信，当时还觉得很好，觉得3G不错。但是4G来了后，大家很少打电话和发短信了，都改为语音、视频、直播、网上购物等生活方式，带火了淘宝、京东、美团、字节跳动等企业。没有跟上节奏的百度，有点摇摇欲坠。<br>自古不变的真理：先入行者吃肉，后入行者喝汤，最后到的买单！<br>（4）人才紧缺、竞争压力小<br><strong>1.5.大数据部门间业务流程分析</strong><br><img src="/2022/04/11/21104/4.png" alt="在这里插入图片描述"><br><strong>1.6.大数据部门内组织结构</strong><br><img src="/2022/04/11/21104/5.png" alt="在这里插入图片描述"></p><h1 id="二、Hadoop概述"><a href="#二、Hadoop概述" class="headerlink" title="二、Hadoop概述"></a>二、Hadoop概述</h1><p><strong>2.1. Hadoop 是什么</strong><br>1） Hadoop是一个由Apache基金会所开发的分布式系统基础架构。<br>2）主要解决，海量数据的存储和海量数据的分析计算问题。<br>3）广义上来说，Hadoop通常是指一个更广泛的概念——Hadoop生态圈。<br><img src="/2022/04/11/21104/6.png" alt="在这里插入图片描述"><br><strong>2.2. Hadoop发展历史（了解）</strong><br>1）Hadoop创始人Doug<br>Cutting，为了实现与Google类似的全文搜索功能，他在Lucene框架基础上进行优化升级，查询引擎和索引引擎。<br>2）2001年年底Lucene成为Apache基金会的一个子项目。<br>3）对于海量数据的场景，Lucene框架面对与Google同样的困难，存储海量数据困难，检索海量速度慢。<br>4）学习和模仿Google解决这些问题的办法 ：微型版Nutch。<br>5）可以说Google是Hadoop的思想之源（Google在大数据方面的三篇论文）</p><blockquote><p>GFS —&gt;HDFS<br>Map-Reduce —&gt;MR<br>BigTable —&gt;HBase</p></blockquote><p>6）2003-2004年，Google公开了部分GFS和MapReduce思想的细节，以此为基础Doug Cutting等人用了2年业余时间实现了DFS和MapReduce机制，使Nutch性能飙升。<br>7）2005 年Hadoop 作为 Lucene的子项目Nutch的一部分正式引入Apache基金会。<br>8）2006 年 3 月份，Map-Reduce和Nutch Distributed File System（NDFS）分别被纳入到 Hadoop<br>项目中，Hadoop就此正式诞生，标志着大数据时代来临。<br>9）名字来源于Doug Cutting儿子的玩具大象<br><strong>2.3. Hadoop三大发行版本（了解）</strong><br>Hadoop 三大发行版本：Apache、Cloudera、Hortonworks。<br>Apache 版本最原始（最基础）的版本，对于入门学习最好。2006<br>Cloudera 内部集成了很多大数据框架，对应产品 CDH。2008<br>Hortonworks 文档较好，对应产品 HDP。2011<br>Hortonworks 现在已经被 Cloudera 公司收购，推出新的品牌 CDP。<br><img src="/2022/04/11/21104/7.png" alt="在这里插入图片描述"><br><img src="/2022/04/11/21104/8.png" alt="在这里插入图片描述"><br><strong>1 ）Apache Hadoop</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">官网地址：http:&#x2F;&#x2F;hadoop.apache.org</span><br><span class="line">下载地址：https:&#x2F;&#x2F;hadoop.apache.org&#x2F;releases.html</span><br></pre></td></tr></table></figure><p><strong>2 ）Cloudera Hadoop</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">官网地址：https:&#x2F;&#x2F;www.cloudera.com&#x2F;downloads&#x2F;cdh</span><br><span class="line">下载地址：https:&#x2F;&#x2F;docs.cloudera.com&#x2F;documentation&#x2F;enterprise&#x2F;6&#x2F;release-notes&#x2F;topics&#x2F;rg_cdh_6_download.html</span><br></pre></td></tr></table></figure><blockquote><p>（1）2008 年成立的 Cloudera 是最早将 Hadoop 商用的公司，为合作伙伴提供Hadoop 的商用解决方案，主要是包括支持、咨询服务、培训。<br>（2 ）2009 年 年 Hadoop 的创始人 Doug Cutting 也加盟Cloudera公司。Cloudera 产品主要为 CDH，Cloudera Manager，Cloudera Support<br>（3）CDH 是 Cloudera 的 Hadoop 发行版，完全开源，比 Apache Hadoop在兼容性，安全性，稳定性上有所增强。Cloudera 的标价为每年每个节点10000 美元。<br>（4）ClouderaManager是集群的软件分发及管理监控平台，可以在几个小时内部署好一个Hadoop 集群，并对集群的节点及服务进行实时监控。</p></blockquote><p><strong>3） Hortonworks Hadoop</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">官网地址：https:&#x2F;&#x2F;hortonworks.com&#x2F;products&#x2F;data-center&#x2F;hdp&#x2F;</span><br><span class="line">下载地址：https:&#x2F;&#x2F;hortonworks.com&#x2F;downloads&#x2F;#data-platform</span><br></pre></td></tr></table></figure><blockquote><p>（1）2011 年成立的 Hortonworks 是雅虎与硅谷风投公司 Benchmark Capital合资组建。<br>（2）公司成立之初就吸纳了大约 25 名至 30 名专究 门研究 Hadoop的雅虎工程师，上述 工程师均在 2005 年开始协助雅虎开发 Hadoop ，贡献了Hadoop80% 的代码。<br>（3）Hortonworks 的主打产品是 Hortonworks DataPlatform（HDP），也同样是 100%开 源的产品，HDP 除常见的项目外还包括了Ambari，一款开源的安装和管理系统。<br>（4）2018 年 Hortonworks 目前 已经被 Cloudera 公司收购。</p></blockquote><p><strong>2.4. Hadoop优势（4高）</strong><br>1）高可靠性：Hadoop底层维护多个数据副本，所以即使Hadoop某个计算元素或存储出现故障，也不会导致数据的丢失。<br><img src="/2022/04/11/21104/9.png" alt="在这里插入图片描述"><br>2）高扩展性：在集群间分配任务数据，可方便的扩展数以千计的节点。<br><img src="/2022/04/11/21104/10.png" alt="在这里插入图片描述"></p><p>3）高效性：在MapReduce的思想下，Hadoop是并行工作的，以加快任务处<br>理速度。<br><img src="/2022/04/11/21104/11.png" alt="在这里插入图片描述"></p><p>4）高容错性：能够自动将失败的任务重新分配。<br><img src="/2022/04/11/21104/12.png" alt="在这里插入图片描述"><br><strong>2.5. Hadoop组成（面试重点）</strong><br><img src="/2022/04/11/21104/13.png" alt="在这里插入图片描述"><br>在 Hadoop1.x 时 代，Hadoop中的MapReduce同时处理业务逻辑运算和资源的调度，耦合性较大。在Hadoop2.x时代，增加了Yarn。Yarn只负责资源 的 调 度 ，MapReduce只负责运算。Hadoop3.x在组成上没有变化。<br><strong>2.5.1 HDFS架构概述</strong><br>Hadoop Distributed File System，简称 HDFS，是一个分布式文件系统。</p><blockquote><p>1）NameNode（nn）：存储文件的元数据，如文件名，文件目录结构，文件属性（生成时间、副本数、文件权限），以及每个文件的块列表和块所在的DataNode等。<br>2）DataNode(dn)：在本地文件系统存储文件块数据，以及块数据的校验和。<br>3）SecondaryNameNode(2nn)：每隔一段时间对NameNode元数据备份。</p></blockquote><p><strong>2.5.2 YARN架构概述</strong><br>Yet Another Resource Negotiator 简称 YARN ，另一种资源协调者，是 Hadoop的资源管理器。<br><img src="/2022/04/11/21104/14.png" alt="在这里插入图片描述"></p><blockquote><p>1）ResourceManager（RM）：整个集群资源（内存、CPU等）的老大<br>3）ApplicationMaster（AM）：单个任务运行的老大<br>2）NodeManager（NM）：单个节点服务器资源老大<br>4）Container：容器，相当一台独立的服务器，里面封装了任务运行所需要的资源，如内存、CPU、磁盘、网络等。<br>说明1：客户端可以有多个<br>说明2：集群上可以运行多个ApplicationMaster<br>说明3：每个NodeManager上可以有多个Container</p></blockquote><p><strong>2.5.3. MapReduce架构概述</strong><br>MapReduce 将计算过程分为两个阶段：Map 和 Reduce</p><blockquote><p>1）Map 阶段并行处理输入数据<br>2）Reduce 阶段对 Map 结果进行汇总</p></blockquote><p><img src="/2022/04/11/21104/15.png" alt="在这里插入图片描述"></p><p><strong>2.5.4. HDFS、YARN、MapReduce三者关系</strong><br><img src="/2022/04/11/21104/16.png" alt="在这里插入图片描述"></p><p><strong>2.6. 大数据技术生态体系</strong><br><img src="/2022/04/11/21104/17.png" alt="在这里插入图片描述"><br>图中涉及的技术名词解释如下：</p><blockquote><p>1）Sqoop：Sqoop 是一款开源的工具，主要用于在 Hadoop、Hive与传统的数据库（MySQL）间进行数据的传递，可以将一个关系型数据库（例如：MySQL，Oracle 等）中的数据导进到 Hadoop 的HDFS 中，也可以将 HDFS 的数据导进到关系型数据库中。 2）Flume：Flume是一个高可用的，高可靠的，分布式的海量日志采集、聚合和传输的系统，Flume支持在日志系统中定制各类数据发送方，用于收集数据；<br>3）Kafka：Kafka 是一种高吞吐量的分布式发布订阅消息系统；<br>4）Spark：Spark是当前最流行的开源大数据内存计算框架。可以基Hadoop<br>上存储的大数据进行计算。<br>5）Flink：Flink是当前最流行的开源大数据内存计算框架。用于实时计算的场景较多。<br>6）Oozie：Oozie 是一个管理 Hadoop作业（job）的工作流程调度管理系统。<br>7）Hbase：HBase是一个分布式的、面向列的开源数据库。HBase不同于一般的关系数据库，它是一个适合于非结构化数据存储的数据库。<br>8）Hive：Hive 是基于 Hadoop的一个数据仓库工具，可以将结构化的数据文件映射为一张数据库表，并提供简单的<br>SQL 查询功能，可以将 SQL 语句转换为 MapReduce任务进行运行。其优点是学习成本低，可以通过类 SQL 语句快速实现简单的MapReduce 统计，不必开发专门的 MapReduce应用，十分适合数据仓库的统计分析。<br>9）ZooKeeper：它是一个针对大型分布式系统的可靠协调系统，提供的功能包括：配置维护、名字服务、分布式同步、组服务等。</p></blockquote><p><strong>2.7. 推荐系统框架图</strong><br><img src="/2022/04/11/21104/18.png" alt="在这里插入图片描述"></p><h1 id="三、Hadoop-运行环境搭建-（开发-重点）"><a href="#三、Hadoop-运行环境搭建-（开发-重点）" class="headerlink" title="三、Hadoop 运行环境搭建 （开发 重点）"></a>三、Hadoop 运行环境搭建 （开发 重点）</h1><h2 id="3-1-模板虚拟机环境准备"><a href="#3-1-模板虚拟机环境准备" class="headerlink" title="3.1 模板虚拟机环境准备"></a>3.1 模板虚拟机环境准备</h2><p><strong>0）安装模板虚拟机</strong><br>IP地址192.168.10.100、主机名称hadoop100、内存4G、硬盘50G</p><p><strong>参考链接:</strong><br><a href="https://blog.csdn.net/m0_52435951/article/details/123987674?spm=1001.2014.3001.5501" target="_blank" rel="noopener">大数据技术——模板虚拟机环境准备</a></p><p><strong>1）hadoop100虚拟机配置要求如下（本文Linux系统全部以CentOS-7.5-x86-1804为例）</strong><br>（1）使用yum安装需要虚拟机可以正常上网，yum安装前可以先测试下虚拟机联网情况<br><img src="/2022/04/11/21104/19.png" alt="在这里插入图片描述"><br>（2）安装 epel-release</p><blockquote><p>注：Extra Packages for Enterprise Linux是为”红帽系”的操作系统提供额外的软件包，适用于RHEL、CentOS 和<br>Scientific Linux。相当于是一个软件仓库，大多数 rpm包在官方repository中是找不到的）</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100 ~]# yum install -y epel-release</span><br></pre></td></tr></table></figure><p><img src="/2022/04/11/21104/20.png" alt="在这里插入图片描述"><br><strong>（3）注意：如果 Linux安装的是最小系统版，还需要安装如下工具；如果安装的是Linux桌面标准版，不需要执行如下操作</strong><br>net-tool：工具包集合，包含 ifconfig 等命令</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100 ~]# yum install -y net-tools</span><br></pre></td></tr></table></figure><p>vim：编辑器</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100 ~]# yum install -y vim</span><br></pre></td></tr></table></figure><p><strong>2 ） 关闭防火墙</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100 ~]# systemctl stop firewalld</span><br><span class="line">[root@hadoop100 ~]# systemctl disable firewalld.service</span><br></pre></td></tr></table></figure><p><img src="/2022/04/11/21104/21.png" alt="在这里插入图片描述"></p><p>注意：在企业开发时，通常单个服务器的防火墙时关闭的。公司整体对外会设置非常安全的防火墙<br><strong>3 ） 创建 xusheng用户 ，并修改 xusheng用户的密码</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100 ~]# useradd xusheng</span><br><span class="line">[root@hadoop100 ~]# passwd xusheng</span><br></pre></td></tr></table></figure><p>自己设置名称和密码<br><strong>4 ） 配置 xusheng用户具有 root 权限 ， 方便 后期加 加 sudo 执行 root<br>权限的命令</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100 ~]# vim &#x2F;etc&#x2F;sudoers</span><br></pre></td></tr></table></figure><p>修改/etc/sudoers 文件，在%wheel 这行下面添加一行，如下所示：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">## Allow root to run any commands anywhere</span><br><span class="line">root ALL&#x3D;(ALL) ALL</span><br><span class="line">## Allows people in group wheel to run all commands</span><br><span class="line">%wheel ALL&#x3D;(ALL) ALL</span><br><span class="line">xushengALL&#x3D;(ALL) NOPASSWD:ALL</span><br></pre></td></tr></table></figure><p><img src="/2022/04/11/21104/22.png" alt="在这里插入图片描述"><br><strong>注意：xusheng这一行不要直接放到 root 行下面，因为所有用户都属于 wheel组，你先配置了 xusheng具有免密功能，但是程序执行到%wheel行时，该功能又被覆盖回需要密码。所以 xusheng要放到%wheel 这行下面。</strong><br><img src="/2022/04/11/21104/23.png" alt="在这里插入图片描述"><br><strong>5 ） 在/opt 目录下创建文件夹 ，并修改所属主和所属组</strong><br>（1）在/opt 目录下创建 module、software 文件夹</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100 ~]# mkdir &#x2F;opt&#x2F;module</span><br><span class="line">[root@hadoop100 ~]# mkdir &#x2F;opt&#x2F;software</span><br></pre></td></tr></table></figure><p><img src="/2022/04/11/21104/24.png" alt="在这里插入图片描述"></p><p>（2）修改 module、software 文件夹的所有者和所属组均为 xusheng用户</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100 ~]# chown xusheng:xusheng &#x2F;opt&#x2F;module</span><br><span class="line">[root@hadoop100 ~]# chown xusheng:xusheng &#x2F;opt&#x2F;software</span><br></pre></td></tr></table></figure><p><img src="/2022/04/11/21104/25.png" alt="在这里插入图片描述"></p><p>（3）查看 module、software 文件夹的所有者和所属组</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[xusheng@hadoop100 opt]$ ll</span><br><span class="line">总用量 0</span><br><span class="line">drwxr-xr-x. 2 xusheng xusheng 6 4月   7 20:36 module</span><br><span class="line">drwxr-xr-x. 2 xusheng xusheng 6 4月   7 20:36 software</span><br></pre></td></tr></table></figure><p><strong>6）卸载虚拟机自带的 JDK</strong><br><strong>注意：如果你的虚拟机是最小化安装不需要执行这一步。</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100 ~]# rpm -qa | grep -i java | xargs -n1 rpm -e --nodeps</span><br></pre></td></tr></table></figure><p>➢ rpm -qa：查询所安装的所有 rpm 软件包<br>➢ grep -i：忽略大小写<br>➢ xargs -n1：表示每次只传递一个参数<br>➢ rpm -e –nodeps：强制卸载软件<br>查询<br><img src="/2022/04/11/21104/26.png" alt="在这里插入图片描述"><br>卸载<br><img src="/2022/04/11/21104/27.png" alt="在这里插入图片描述"></p><p><strong>7 ）重启虚拟机</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100 ~]# reboot</span><br></pre></td></tr></table></figure><h2 id="3-2-克隆虚拟机"><a href="#3-2-克隆虚拟机" class="headerlink" title="3.2 克隆虚拟机"></a>3.2 克隆虚拟机</h2><p><strong>1 ） 利用模板机 hadoop100 ，克隆 三台虚拟机： ：hadoop102 hadoop103<br>hadoop104</strong><br>注意：克隆时，要先关闭 hadoop100<br>hadoop100右键–》管理–》克隆<br><strong>2 ）修改 克隆机 IP ，以下以 hadoop102 举例说明</strong><br>（1）修改克隆虚拟机的静态 IP</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop100 ~]# vim &#x2F;etc&#x2F;sysconfig&#x2F;network-scripts&#x2F;ifcfg- ens33</span><br><span class="line">[root@hadoop100 ~]# vim &#x2F;etc&#x2F;hostname</span><br><span class="line">[root@hadoop100 ~]# vim &#x2F;etc&#x2F;hosts</span><br></pre></td></tr></table></figure><p>改成</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">DEVICE&#x3D;ens33</span><br><span class="line">TYPE&#x3D;Ethernet</span><br><span class="line">ONBOOT&#x3D;yes</span><br><span class="line">BOOTPROTO&#x3D;static</span><br><span class="line">NAME&#x3D;&quot;ens33&quot;</span><br><span class="line">IPADDR&#x3D;192.168.10.102</span><br><span class="line">PREFIX&#x3D;24</span><br><span class="line">GATEWAY&#x3D;192.168.10.2</span><br><span class="line">DNS1&#x3D;192.168.10.2</span><br></pre></td></tr></table></figure><p>（2）查看 Linux 虚拟机的虚拟网络编辑器，编辑-&gt;虚拟网络编辑器-&gt;VMnet8</p><h2 id="3-3-在hadoop102安装JDK"><a href="#3-3-在hadoop102安装JDK" class="headerlink" title="3.3 在hadoop102安装JDK"></a>3.3 在hadoop102安装JDK</h2><p><strong>1）卸载现有JDK</strong><br>注意：安装JDK前，一定确保提前删除了虚拟机自带的JDK。详细步骤见问文档3.1（6）节中卸载JDK步骤。<br><strong>2）用XShell传输工具将JDK导入到opt目录下面的software文件夹下面</strong></p><p><img src="/2022/04/11/21104/28.png" alt="在这里插入图片描述"><br><img src="/2022/04/11/21104/29.png" alt="在这里插入图片描述"><br><strong>3）在Linux系统下的opt目录中查看软件包是否导入成功</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[xusheng@hadoop102 ~]$ ls &#x2F;opt&#x2F;software&#x2F;</span><br></pre></td></tr></table></figure><p>看到如下结果：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">jdk-8u212-linux-x64.tar.gz</span><br></pre></td></tr></table></figure><p><strong>4）解压JDK到/opt/module目录下</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[xusheng@hadoop102 software]$ tar -zxvf jdk-8u212-linux-x64.tar.gz -C &#x2F;opt&#x2F;module&#x2F;</span><br></pre></td></tr></table></figure><p><img src="/2022/04/11/21104/30.png" alt="在这里插入图片描述"></p><p><strong>5）配置JDK环境变量</strong><br>（1）新建/etc/profile.d/my_env.sh文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[xusheng@hadoop102 ~]$ sudo vim &#x2F;etc&#x2F;profile.d&#x2F;my_env.sh</span><br></pre></td></tr></table></figure><p>添加如下内容</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">#JAVA_HOME</span><br><span class="line">export JAVA_HOME&#x3D;&#x2F;opt&#x2F;module&#x2F;jdk1.8.0_212</span><br><span class="line">export PATH&#x3D;$PATH:$JAVA_HOME&#x2F;bin</span><br></pre></td></tr></table></figure><p>（2）保存后退出<br>:wq<br>（3）source一下/etc/profile文件，让新的环境变量PATH生效</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[xusheng@hadoop102 ~]$ source &#x2F;etc&#x2F;profile</span><br></pre></td></tr></table></figure><p><strong>6）测试JDK是否安装成功</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[xusheng@hadoop102 ~]$ java -version</span><br></pre></td></tr></table></figure><p>如果能看到以下结果，则代表Java安装成功。<br>java version “1.8.0_212”<br><img src="/2022/04/11/21104/31.png" alt="在这里插入图片描述"></p><p>注意：重启（如果java -version可以用就不用重启）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[xusheng@hadoop102 ~]$ sudo reboot</span><br></pre></td></tr></table></figure><h2 id="3-4-在hadoop102安装Hadoop"><a href="#3-4-在hadoop102安装Hadoop" class="headerlink" title="3.4 在hadoop102安装Hadoop"></a>3.4 在hadoop102安装Hadoop</h2><p>Hadoop下载地址：<a href="https://archive.apache.org/dist/hadoop/common/hadoop-3.1.3/" target="_blank" rel="noopener">https://archive.apache.org/dist/hadoop/common/hadoop-3.1.3/</a><br><strong>1）用XShell文件传输工具将hadoop-3.1.3.tar.gz导入到opt目录下面的software文件夹下面</strong></p><p><img src="/2022/04/11/21104/32.png" alt="在这里插入图片描述"><br><strong>2）进入到Hadoop安装包路径下</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[xusheng@hadoop102 ~]$ cd &#x2F;opt&#x2F;software&#x2F;</span><br></pre></td></tr></table></figure><p><strong>3）解压安装文件到/opt/module下面</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[xusheng@hadoop102 software]$ tar -zxvf hadoop-3.1.3.tar.gz -C &#x2F;opt&#x2F;module&#x2F;</span><br></pre></td></tr></table></figure><p><strong>4）查看是否解压成功</strong></p><blockquote><p>[xusheng@hadoop102 software]$ ls /opt/module/ hadoop-3.1.3</p></blockquote><p><strong>5）将Hadoop添加到环境变量</strong><br>（1）获取Hadoop安装路径</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[xusheng@hadoop102 hadoop-3.1.3]$ pwd</span><br><span class="line">&#x2F;opt&#x2F;module&#x2F;hadoop-3.1.3</span><br></pre></td></tr></table></figure><p>（2）打开/etc/profile.d/my_env.sh文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[xusheng@hadoop102 hadoop-3.1.3]$ sudo vim &#x2F;etc&#x2F;profile.d&#x2F;my_env.sh</span><br></pre></td></tr></table></figure><p>在my_env.sh文件末尾添加如下内容：（shift+g）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">#HADOOP_HOME</span><br><span class="line">export HADOOP_HOME&#x3D;&#x2F;opt&#x2F;module&#x2F;hadoop-3.1.3</span><br><span class="line">export PATH&#x3D;$PATH:$HADOOP_HOME&#x2F;bin</span><br><span class="line">export PATH&#x3D;$PATH:$HADOOP_HOME&#x2F;sbin</span><br></pre></td></tr></table></figure><p>保存并退出： :wq<br>（3）让修改后的文件生效</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[xusheng@hadoop102 hadoop-3.1.3]$ source &#x2F;etc&#x2F;profile</span><br></pre></td></tr></table></figure><p><strong>6）测试是否安装成功</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[xusheng@hadoop102 hadoop-3.1.3]$ hadoop version</span><br><span class="line">Hadoop 3.1.3</span><br></pre></td></tr></table></figure><p><strong>7）重启（如果Hadoop命令不能用再重启虚拟机）</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[xusheng@hadoop102 hadoop-3.1.3]$ sudo reboot</span><br></pre></td></tr></table></figure><p><img src="/2022/04/11/21104/33.png" alt="在这里插入图片描述"></p><h2 id="3-5-Hadoop目录结构"><a href="#3-5-Hadoop目录结构" class="headerlink" title="3.5 Hadoop目录结构"></a>3.5 Hadoop目录结构</h2><p><strong>1）查看Hadoop目录结构</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[xusheng@hadoop102 hadoop-3.1.3]$ ll</span><br><span class="line">总用量 176</span><br><span class="line">drwxr-xr-x. 2 xusheng xusheng    183 9月  12 2019 bin</span><br><span class="line">drwxr-xr-x. 3 xusheng xusheng     20 9月  12 2019 etc</span><br><span class="line">drwxr-xr-x. 2 xusheng xusheng    106 9月  12 2019 include</span><br><span class="line">drwxr-xr-x. 3 xusheng xusheng     20 9月  12 2019 lib</span><br><span class="line">drwxr-xr-x. 4 xusheng xusheng    288 9月  12 2019 libexec</span><br><span class="line">-rw-rw-r--. 1 xusheng xusheng 147145 9月   4 2019 LICENSE.txt</span><br><span class="line">-rw-rw-r--. 1 xusheng xusheng  21867 9月   4 2019 NOTICE.txt</span><br><span class="line">-rw-rw-r--. 1 xusheng xusheng   1366 9月   4 2019 README.txt</span><br><span class="line">drwxr-xr-x. 3 xusheng xusheng   4096 9月  12 2019 sbin</span><br><span class="line">drwxr-xr-x. 4 xusheng xusheng     31 9月  12 2019 share</span><br><span class="line">[xusheng@hadoop102 hadoop-3.1.3]$</span><br></pre></td></tr></table></figure><p><strong>2）重要目录</strong><br>（1）bin目录：存放对Hadoop相关服务（hdfs，yarn，mapred）进行操作的脚本<br>（2）etc目录：Hadoop的配置文件目录，存放Hadoop的配置文件<br>（3）lib目录：存放Hadoop的本地库（对数据进行压缩解压缩功能）<br>（4）sbin目录：存放启动或停止Hadoop相关服务的脚本<br>（5）share目录：存放Hadoop的依赖jar包、文档、和官方案例</p><h1 id="四、Hadoop运行模式"><a href="#四、Hadoop运行模式" class="headerlink" title="四、Hadoop运行模式"></a>四、Hadoop运行模式</h1><p>1）Hadoop官方网站：<a href="http://hadoop.apache.org/" target="_blank" rel="noopener">http://hadoop.apache.org/</a><br>2）Hadoop运行模式包括：本地模式、伪分布式模式以及完全分布式模式。</p><blockquote><p>本地模式：单机运行，只是用来演示一下官方案例。生产环境不用。<br>伪分布式模式：也是单机运行，但是具备Hadoop集群的所有功能，一台服务器模拟一个分布式的环境。个别缺钱的公司用来测试，生产环境不用。<br>完全分布式模式：多台服务器组成分布式环境。生产环境使用。</p></blockquote><h2 id="4-1-本地运行模式（官方WordCount）"><a href="#4-1-本地运行模式（官方WordCount）" class="headerlink" title="4.1 本地运行模式（官方WordCount）"></a>4.1 本地运行模式（官方WordCount）</h2><p><strong>1）创建在hadoop-3.1.3文件下面创建一个wcinput文件夹</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[xusheng@hadoop102 hadoop-3.1.3]$ mkdir wcinput</span><br></pre></td></tr></table></figure><p><strong>2）在wcinput文件下创建一个word.txt文件</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[xusheng@hadoop102 hadoop-3.1.3]$ cd wcinput</span><br></pre></td></tr></table></figure><p><strong>3）编辑word.txt文件</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[xusheng@hadoop102 wcinput]$ vim word.txt</span><br></pre></td></tr></table></figure><p>在文件中输入如下内容</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hadoop yarn</span><br><span class="line">hadoop mapreduce</span><br><span class="line">xusheng</span><br><span class="line">xusheng</span><br></pre></td></tr></table></figure><p>保存退出：:wq<br><strong>4）回到Hadoop目录/opt/module/hadoop-3.1.3</strong><br><strong>5）执行程序</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[xusheng@hadoop102 hadoop-3.1.3]$ hadoop jar share&#x2F;hadoop&#x2F;mapreduce&#x2F;hadoop-mapreduce-examples-3.1.3.jar wordcount wcinput wcoutput</span><br></pre></td></tr></table></figure><p><strong>6）查看结果</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[xusheng@hadoop102 hadoop-3.1.3]$ cat wcoutput&#x2F;part-r-00000</span><br></pre></td></tr></table></figure><p>看到如下结果：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">xusheng2</span><br><span class="line">hadoop  2</span><br><span class="line">mapreduce       1</span><br><span class="line">yarn    1</span><br></pre></td></tr></table></figure><p><img src="/2022/04/11/21104/34.png" alt="在这里插入图片描述"></p><h2 id="4-2-完全分布式运行模式（开发重点）"><a href="#4-2-完全分布式运行模式（开发重点）" class="headerlink" title="4.2 完全分布式运行模式（开发重点）"></a>4.2 完全分布式运行模式（开发重点）</h2><p><strong>分析：</strong><br>1）准备3台客户机（关闭防火墙、静态IP、主机名称）<br>2）安装JDK<br>3）配置环境变量<br>4）安装Hadoop<br>5）配置环境变量<br>6）配置集群<br>7）单点启动<br>8）配置ssh<br>9）群起并测试集群</p><h3 id="4-2-1-虚拟机准备"><a href="#4-2-1-虚拟机准备" class="headerlink" title="4.2.1 虚拟机准备"></a>4.2.1 虚拟机准备</h3><p>详见2.1、2.2两节。</p><h3 id="4-2-2-编写集群分发脚本xsync"><a href="#4-2-2-编写集群分发脚本xsync" class="headerlink" title="4.2.2 编写集群分发脚本xsync"></a>4.2.2 编写集群分发脚本xsync</h3><p><strong>1）scp（secure copy）安全拷贝</strong><br>（1）scp 定义<br>scp 可以实现服务器与服务器之间的数据拷贝。（from server1 to server2）<br>（2）基本语法</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scp -r  $pdir&#x2F;$fname        $user@$host:$pdir&#x2F;$fname</span><br></pre></td></tr></table></figure><p>命令 递归 要拷贝的文件路径/名称 目的地用户@主机:目的地路径/名称<br>（3）案例实操<br>➢ 前提：在 hadoop102、hadoop103、hadoop104都已经创建好的/opt/module、/opt/software两个目录，并且已经把这两个目录修改为 xusheng:xusheng</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[xusheng@hadoop102 ~]$ sudo chown xusheng:xusheng -R &#x2F;opt&#x2F;module</span><br></pre></td></tr></table></figure><p>（a）在 hadoop102 上，将 hadoop102 中/opt/module/jdk1.8.0_212目录拷贝到hadoop103 上。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[xusheng@hadoop102 ~]$ scp -r &#x2F;opt&#x2F;module&#x2F;jdk1.8.0_212</span><br><span class="line">xusheng@hadoop103:&#x2F;opt&#x2F;module</span><br></pre></td></tr></table></figure><p>（b）在 hadoop103 上，将 hadoop102 中/opt/module/hadoop-3.1.3目录拷贝到hadoop103 上。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[xusheng@hadoop103 ~]$ scp -r</span><br><span class="line">xusheng@hadoop102:&#x2F;opt&#x2F;module&#x2F;hadoop-3.1.3 &#x2F;opt&#x2F;module&#x2F;</span><br></pre></td></tr></table></figure><p><img src="/2022/04/11/21104/35.png" alt="在这里插入图片描述"></p><p>（c）在 hadoop103 上操作，将 hadoop102 中/opt/module目录下所有目录拷贝到hadoop104 上。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[xusheng@hadoop103 opt]$ scp -r</span><br><span class="line">xusheng@hadoop102:&#x2F;opt&#x2F;module&#x2F;*</span><br><span class="line">xusheng@hadoop104:&#x2F;opt&#x2F;module</span><br></pre></td></tr></table></figure><p><img src="/2022/04/11/21104/36.png" alt="在这里插入图片描述"></p><p><strong>2 ）rsync 远程 同步</strong></p><blockquote><p>rsync主要用于备份和镜像。具有速度快、避免复制相同内容和支持符号链接的优点。<br>rsync 和 scp 区别：用 rsync做文件的复制要比 scp 的速度快，rsync只对差异文件做更新。scp 是把所有文件都复制过去。</p></blockquote><p>（1）基本语法</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rsync -av $pdir&#x2F;$fname $user@$host:$pdir&#x2F;$fname</span><br></pre></td></tr></table></figure><p>命令 选项参数 要拷贝的文件路径/名称 目的地用户@主机:目的地路径/名称<br>选项参数说明</p><blockquote><p>选项 功能<br>-a 归档拷贝<br>-v 显示复制过程</p></blockquote><p>（2）案例实操<br>（a）删除 hadoop103 中/opt/module/hadoop-3.1.3/wcinput</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[xusheng@hadoop103 hadoop-3.1.3]$ rm -rf wcinput&#x2F;</span><br></pre></td></tr></table></figure><p>（b）同步 hadoop102 中的/opt/module/hadoop-3.1.3 到 hadoop103</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[xusheng@hadoop102 module]$ rsync -av hadoop-3.1.3&#x2F;</span><br><span class="line">xusheng@hadoop103:&#x2F;opt&#x2F;module&#x2F;hadoop-3.1.3&#x2F;</span><br></pre></td></tr></table></figure><p><strong>3 ）xsync 集群分发 脚本</strong><br>（1）需求：循环复制文件到所有节点的相同目录下<br>（2）需求分析：<br>（a）rsync 命令原始拷贝：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rsync -av &#x2F;opt&#x2F;module xusheng@hadoop103:&#x2F;opt&#x2F;</span><br></pre></td></tr></table></figure><p>（b）期望脚本：<br>xsync 要同步的文件名称<br>（c）期望脚本在任何路径都能使用（脚本放在声明了全局环境变量的路径）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[xusheng@hadoop102 ~]$ echo $PATH</span><br><span class="line">&#x2F;usr&#x2F;local&#x2F;bin:&#x2F;usr&#x2F;bin:&#x2F;usr&#x2F;local&#x2F;sbin:&#x2F;usr&#x2F;sbin:&#x2F;home&#x2F;xusheng&#x2F;.local&#x2F;bin:&#x2F;home&#x2F;xusheng&#x2F;bin:&#x2F;opt&#x2F;module&#x2F;jdk1.8.0_212&#x2F;bin</span><br></pre></td></tr></table></figure><p>（3）脚本实现<br>（a）在/home/xusheng/bin 目录下创建 xsync 文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[xusheng@hadoop102 opt]$ cd &#x2F;home&#x2F;xusheng</span><br><span class="line">[xusheng@hadoop102 ~]$ mkdir bin</span><br><span class="line">[xusheng@hadoop102 ~]$ cd bin&#x2F;</span><br><span class="line">[xusheng@hadoop102 bin]$ vim xsync</span><br></pre></td></tr></table></figure><p>在该文件中编写如下代码</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">#!&#x2F;bin&#x2F;bash</span><br><span class="line">#1. 判断参数个数</span><br><span class="line">if [ $# -lt 1 ]</span><br><span class="line">then</span><br><span class="line">    echo Not Enough Arguement!</span><br><span class="line">    exit;</span><br><span class="line">fi</span><br><span class="line">#2. 遍历集群所有机器</span><br><span class="line">for host in hadoop102 hadoop103 hadoop104</span><br><span class="line">do</span><br><span class="line">    echo &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D; $host &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line">    #3. 遍历所有目录，挨个发送</span><br><span class="line">    for file in $@</span><br><span class="line">    do</span><br><span class="line">        #4. 判断文件是否存在</span><br><span class="line">        if [ -e $file ]</span><br><span class="line">            then</span><br><span class="line">                #5. 获取父目录</span><br><span class="line">                pdir&#x3D;$(cd -P $(dirname $file); pwd)</span><br><span class="line">                #6. 获取当前文件的名称</span><br><span class="line">                fname&#x3D;$(basename $file)</span><br><span class="line">                ssh $host &quot;mkdir -p $pdir&quot;</span><br><span class="line">                rsync -av $pdir&#x2F;$fname $host:$pdir</span><br><span class="line">            else</span><br><span class="line">                echo $file does not exists!</span><br><span class="line">        fi</span><br><span class="line">    done</span><br><span class="line">done</span><br></pre></td></tr></table></figure><p>（b）修改脚本 xsync 具有执行权限</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[xusheng@hadoop102 bin]$ chmod +x xsync</span><br></pre></td></tr></table></figure><p>（c）测试脚本</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[xusheng@hadoop102 ~]$ xsync &#x2F;home&#x2F;xusheng&#x2F;bin</span><br></pre></td></tr></table></figure><p>（d）将脚本复制到/bin 中，以便全局调用</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[xusheng@hadoop102 bin]$ sudo cp xsync &#x2F;bin&#x2F;</span><br></pre></td></tr></table></figure><p>（e）同步环境变量配置（root 所有者）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[xusheng@hadoop102 ~]$ sudo .&#x2F;bin&#x2F;xsync &#x2F;etc&#x2F;profile.d&#x2F;my_env.sh</span><br></pre></td></tr></table></figure><p><img src="/2022/04/11/21104/37.png" alt="在这里插入图片描述"><br><img src="/2022/04/11/21104/38.png" alt="在这里插入图片描述"></p><p>注意：如果用了 sudo，那么 xsync 一定要给它的路径补全。让环境变量生效</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[xusheng@hadoop103 bin]$ source &#x2F;etc&#x2F;profile</span><br><span class="line">[xusheng@hadoop104 opt]$ source &#x2F;etc&#x2F;profile</span><br></pre></td></tr></table></figure><p><img src="/2022/04/11/21104/39.png" alt="在这里插入图片描述"></p><h3 id="4-2-3-SSH无密登录配置"><a href="#4-2-3-SSH无密登录配置" class="headerlink" title="4.2.3 SSH无密登录配置"></a>4.2.3 SSH无密登录配置</h3><p><strong>1 ）置 配置 ssh</strong><br>（1）基本语法</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh 另一台电脑的 IP 地址</span><br></pre></td></tr></table></figure><p>（2）ssh 连接时出现 Host key verification failed 的解决方法</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[xusheng@hadoop102 ~]$ ssh hadoop103</span><br></pre></td></tr></table></figure><p>➢ 如果出现如下内容</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Are you sure you want to continue connecting (yes&#x2F;no)?</span><br></pre></td></tr></table></figure><p>➢ 输入 yes，并回车<br>（3）退回到 hadoop102</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[xusheng@hadoop103 ~]$ exit</span><br></pre></td></tr></table></figure><p><strong>2 ） 无密钥配置</strong><br>（1）免密登录原理<br><img src="/2022/04/11/21104/40.png" alt="在这里插入图片描述"><br>（2）生成公钥和私钥</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[xusheng@hadoop102 .ssh]$ pwd</span><br><span class="line">&#x2F;home&#x2F;xusheng&#x2F;.ssh</span><br><span class="line">[xusheng@hadoop102 .ssh]$ ssh-keygen -t rsa</span><br></pre></td></tr></table></figure><p><strong>然后敲（三个回车）</strong>，就会生成两个文件<br>id_rsa（私钥）、id_rsa.pub（公钥）<br><img src="/2022/04/11/21104/41.png" alt="在这里插入图片描述"><br><img src="/2022/04/11/21104/42.png" alt="在这里插入图片描述"></p><p>（3）将公钥拷贝到要免密登录的目标机器上</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[xusheng@hadoop102 .ssh]$ ssh-copy-id hadoop102</span><br><span class="line">[xusheng@hadoop102 .ssh]$ ssh-copy-id hadoop103</span><br><span class="line">[xusheng@hadoop102 .ssh]$ ssh-copy-id hadoop104</span><br></pre></td></tr></table></figure><p><strong>注意：</strong><br>还需要在 hadoop103 上采用 xusheng账号配置一下无密登录hadoop102、hadoop103、hadoop104 服务器上。<br>还需要在 hadoop104 上采用xusheng账号配置一下无密登录hadoop102、hadoop103、hadoop104 服务器上。<br>还需要在 hadoop102 上采用 root 账号，配置一下无密登录到hadoop102、hadoop103、hadoop104；<br><strong>3 ）.ssh 文件夹下 （~/.ssh ）</strong><br><img src="/2022/04/11/21104/43.png" alt="在这里插入图片描述"></p><h3 id="4-2-4-集群配置"><a href="#4-2-4-集群配置" class="headerlink" title="4.2.4 集群配置"></a>4.2.4 集群配置</h3><p><strong>1 ） 集群部署规划</strong><br>注意：<br>➢ NameNode 和 SecondaryNameNode 不要安装在同一台服务器<br>➢ ResourceManager 也很消耗内存，不要和 NameNode、SecondaryNameNode<br>配置在同一台机器上。<br><img src="/2022/04/11/21104/44.png" alt="在这里插入图片描述"><br><strong>2 ）配置文件说明</strong><br>Hadoop<br>配置文件分两类：默认配置文件和自定义配置文件，只有用户想修改某一默认配置值时，才需要修改自定义配置文件，更改相应属性值。<br>（1）默认配置文件：<br><img src="/2022/04/11/21104/45.png" alt="在这里插入图片描述"></p><p>（2）自定义配置文件：<br>core-site.xml 、hdfs-site.xml 、yarn-site.xml 、mapred-site.xml<br>四个配置文件存放在</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$HADOOP_HOME&#x2F;etc&#x2F;hadoop</span><br></pre></td></tr></table></figure><p>这个路径上，用户可以根据项目需求重新进行修改配置。<br><strong>3 ） 配置 集群</strong><br>（1）核心配置文件<br>配置 core-site.xml</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[xusheng@hadoop102 ~]$ cd $HADOOP_HOME&#x2F;etc&#x2F;hadoop</span><br><span class="line">[xusheng@hadoop102 hadoop]$ vim core-site.xml</span><br></pre></td></tr></table></figure><p>文件内容如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version&#x3D;&quot;1.0&quot; encoding&#x3D;&quot;UTF-8&quot;?&gt;</span><br><span class="line">&lt;?xml-stylesheet type&#x3D;&quot;text&#x2F;xsl&quot; href&#x3D;&quot;configuration.xsl&quot;?&gt;</span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;!-- 指定 NameNode 的地址 --&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;fs.defaultFS&lt;&#x2F;name&gt;</span><br><span class="line">        &lt;value&gt;hdfs:&#x2F;&#x2F;hadoop102:8020&lt;&#x2F;value&gt;</span><br><span class="line">    &lt;&#x2F;property&gt;</span><br><span class="line">    &lt;!-- 指定 hadoop 数据的存储目录 --&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;hadoop.tmp.dir&lt;&#x2F;name&gt;</span><br><span class="line">        &lt;value&gt;&#x2F;opt&#x2F;module&#x2F;hadoop-3.1.3&#x2F;data&lt;&#x2F;value&gt;</span><br><span class="line">    &lt;&#x2F;property&gt;</span><br><span class="line">    &lt;!-- 配置 HDFS 网页登录使用的静态用户为 xusheng--&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;hadoop.http.staticuser.user&lt;&#x2F;name&gt;</span><br><span class="line">        &lt;value&gt;xusheng&lt;&#x2F;value&gt;</span><br><span class="line">    &lt;&#x2F;property&gt;</span><br><span class="line">&lt;&#x2F;configuration&gt;</span><br></pre></td></tr></table></figure><p>（2）HDFS 配置文件<br>配置 hdfs-site.xml</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[xusheng@hadoop102 hadoop]$ vim hdfs-site.xml</span><br></pre></td></tr></table></figure><p>文件内容如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version&#x3D;&quot;1.0&quot; encoding&#x3D;&quot;UTF-8&quot;?&gt;</span><br><span class="line">&lt;?xml-stylesheet type&#x3D;&quot;text&#x2F;xsl&quot; href&#x3D;&quot;configuration.xsl&quot;?&gt;</span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;!-- nn web 端访问地址--&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.namenode.http-address&lt;&#x2F;name&gt;</span><br><span class="line">        &lt;value&gt;hadoop102:9870&lt;&#x2F;value&gt;</span><br><span class="line">    &lt;&#x2F;property&gt;</span><br><span class="line">    &lt;!-- 2nn web 端访问地址--&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.namenode.secondary.http-address&lt;&#x2F;name&gt;</span><br><span class="line">        &lt;value&gt;hadoop104:9868&lt;&#x2F;value&gt;</span><br><span class="line">    &lt;&#x2F;property&gt;</span><br><span class="line">&lt;&#x2F;configuration&gt;</span><br></pre></td></tr></table></figure><p>（3）YARN 配置文件<br>配置 yarn-site.xml</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[xusheng@hadoop102 hadoop]$ vim yarn-site.xml</span><br></pre></td></tr></table></figure><p>文件内容如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version&#x3D;&quot;1.0&quot; encoding&#x3D;&quot;UTF-8&quot;?&gt;</span><br><span class="line">&lt;?xml-stylesheet type&#x3D;&quot;text&#x2F;xsl&quot; href&#x3D;&quot;configuration.xsl&quot;?&gt;</span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;!-- 指定 MR 走 shuffle --&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.nodemanager.aux-services&lt;&#x2F;name&gt;</span><br><span class="line">        &lt;value&gt;mapreduce_shuffle&lt;&#x2F;value&gt;</span><br><span class="line">    &lt;&#x2F;property&gt;</span><br><span class="line">    &lt;!-- 指定 ResourceManager 的地址--&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.resourcemanager.hostname&lt;&#x2F;name&gt;</span><br><span class="line">        &lt;value&gt;hadoop103&lt;&#x2F;value&gt;</span><br><span class="line">    &lt;&#x2F;property&gt;</span><br><span class="line">    &lt;!-- 环境变量的继承 --&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.nodemanager.env-whitelist&lt;&#x2F;name&gt;</span><br><span class="line">    &lt;value&gt;JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAP</span><br><span class="line">    RED_HOME&lt;&#x2F;value&gt;</span><br><span class="line">    &lt;&#x2F;property&gt;</span><br><span class="line">&lt;&#x2F;configuration&gt;</span><br></pre></td></tr></table></figure><p>（4）MapReduce 配置文件<br>配置 mapred-site.xml</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[xusheng@hadoop102 hadoop]$ vim mapred-site.xml</span><br></pre></td></tr></table></figure><p>文件内容如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version&#x3D;&quot;1.0&quot; encoding&#x3D;&quot;UTF-8&quot;?&gt;</span><br><span class="line">&lt;?xml-stylesheet type&#x3D;&quot;text&#x2F;xsl&quot; href&#x3D;&quot;configuration.xsl&quot;?&gt;</span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;!-- 指定 MapReduce 程序运行在 Yarn 上 --&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;mapreduce.framework.name&lt;&#x2F;name&gt;</span><br><span class="line">        &lt;value&gt;yarn&lt;&#x2F;value&gt;</span><br><span class="line">    &lt;&#x2F;property&gt;</span><br><span class="line">&lt;&#x2F;configuration&gt;</span><br><span class="line">123456789</span><br></pre></td></tr></table></figure><p><strong>4 ） 在集群上分发配置好的 Hadoop 配置文件</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[xusheng@hadoop102 hadoop]$ xsync &#x2F;opt&#x2F;module&#x2F;hadoop-3.1.3&#x2F;etc&#x2F;hadoop&#x2F;</span><br></pre></td></tr></table></figure><p><img src="/2022/04/11/21104/46.png" alt="在这里插入图片描述"></p><p>5 ）去 去 103 和 104 上 查看文件分发情况</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[xusheng@hadoop103 ~]$ cat &#x2F;opt&#x2F;module&#x2F;hadoop-3.1.3&#x2F;etc&#x2F;hadoop&#x2F;core-site.xml</span><br><span class="line">[xusheng@hadoop104 ~]$ cat &#x2F;opt&#x2F;module&#x2F;hadoop-3.1.3&#x2F;etc&#x2F;hadoop&#x2F;core-site.xml</span><br></pre></td></tr></table></figure><h3 id="4-2-5-群起集群"><a href="#4-2-5-群起集群" class="headerlink" title="4.2.5 群起集群"></a>4.2.5 群起集群</h3><p><strong>1 ）置 配置 workers</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[xusheng@hadoop102 hadoop]$ vim &#x2F;opt&#x2F;module&#x2F;hadoop-3.1.3&#x2F;etc&#x2F;hadoop&#x2F;workers</span><br></pre></td></tr></table></figure><p>在该文件中增加如下内容：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hadoop102</span><br><span class="line">hadoop103</span><br><span class="line">hadoop104</span><br></pre></td></tr></table></figure><p><img src="/2022/04/11/21104/47.png" alt="在这里插入图片描述"></p><p>注意：该文件中添加的内容结尾不允许有空格，文件中不允许有空行。<br>同步所有节点配置文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[xusheng@hadoop102 hadoop]$ xsync &#x2F;opt&#x2F;module&#x2F;hadoop-3.1.3&#x2F;etc</span><br></pre></td></tr></table></figure><p><strong>2 ） 启动集群</strong><br>（1） 如果集群是第一次启动，需要在 hadoop102 节点格式化NameNode（注意：格式化 NameNode，会产生新的集群 id，导致 NameNode 和DataNode 的集群 id不一致，集群找不到已往数据。如果集群在运行过程中报错，需要重新格式化<br>NameNode 的话，一定要先停止 namenode 和 datanode进程，并且要删除所有机器的 data 和 logs 录，然后再进行格式化。)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[xusheng@hadoop102 hadoop-3.1.3]$ hdfs namenode -format</span><br></pre></td></tr></table></figure><p>（2）启动 HDFS</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[xusheng@hadoop102 hadoop-3.1.3]$ sbin&#x2F;start-dfs.sh</span><br></pre></td></tr></table></figure><p>（3） 在配置了 ResourceManager 的节点 （hadoop103 ）启动 YARN</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[xusheng@hadoop103 hadoop-3.1.3]$ sbin&#x2F;start-yarn.sh</span><br></pre></td></tr></table></figure><p><img src="/2022/04/11/21104/48.png" alt="在这里插入图片描述"></p><p>（4）Web 端查看 HDFS 的 NameNode</p><blockquote><p>（a）浏览器中输入：<a href="http://hadoop102:9870\" target="_blank" rel="noopener">http://hadoop102:9870\</a><br>（b）查看 HDFS 上存储的数据信息</p></blockquote><p>（5）Web 端查看 YARN 的 ResourceManager</p><blockquote><p>（a）浏览器中输入：<a href="http://hadoop103:8088\" target="_blank" rel="noopener">http://hadoop103:8088\</a><br>（b）查看 YARN 上运行的 Job 信息</p></blockquote><p><img src="/2022/04/11/21104/49.png" alt="在这里插入图片描述"></p><p><strong>3 ） 集群基本测试</strong><br>（1）上传文件到集群<br>➢ 上传小文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[xusheng@hadoop102 ~]$ hadoop fs -mkdir &#x2F;input</span><br><span class="line">[xusheng@hadoop102 ~]$ hadoop fs -put</span><br><span class="line">$HADOOP_HOME&#x2F;wcinput&#x2F;word.txt &#x2F;input</span><br></pre></td></tr></table></figure><p><img src="/2022/04/11/21104/50.png" alt="在这里插入图片描述"></p><p><img src="/2022/04/11/21104/51.png" alt="在这里插入图片描述"></p><p>➢ 上传大文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[xusheng@hadoop102 ~]$ hadoop fs -put &#x2F;opt&#x2F;software&#x2F;jdk-8u212-linux-x64.tar.gz &#x2F;</span><br></pre></td></tr></table></figure><p>（2）上传文件后查看文件存放在什么位置<br>➢ 查看 HDFS 文件存储路径</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[xusheng@hadoop102 subdir0]$ pwd</span><br><span class="line">&#x2F;opt&#x2F;module&#x2F;hadoop-3.1.3&#x2F;data&#x2F;dfs&#x2F;data&#x2F;current&#x2F;BP-1436128598-</span><br><span class="line">192.168.10.102-1610603650062&#x2F;current&#x2F;finalized&#x2F;subdir0&#x2F;subdir0</span><br></pre></td></tr></table></figure><p>➢ 查看 HDFS 在磁盘存储文件内容</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[xusheng@hadoop102 subdir0]$ cat blk_1073741825</span><br><span class="line">hadoop yarn</span><br><span class="line">hadoop mapreduce</span><br><span class="line">xusheng</span><br><span class="line">xusheng</span><br></pre></td></tr></table></figure><p><img src="/2022/04/11/21104/52.png" alt="在这里插入图片描述"></p><p>（3）拼接</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">-rw-rw-r--. 1 xusheng xusheng 134217728 5 月 23 16:01 blk_1073741836</span><br><span class="line">-rw-rw-r--. 1 xusheng xusheng 1048583 5 月 23 16:01 blk_1073741836_1012.meta</span><br><span class="line">-rw-rw-r--. 1 xusheng xusheng 63439959 5 月 23 16:01 blk_1073741837</span><br><span class="line">-rw-rw-r--. 1 xusheng xusheng 495635 5 月 23 16:01 blk_1073741837_1013.meta</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[xusheng@hadoop102 subdir0]$ cat blk_1073741836&gt;&gt;tmp.tar.gz</span><br><span class="line">[xusheng@hadoop102 subdir0]$ cat blk_1073741837&gt;&gt;tmp.tar.gz</span><br><span class="line">[xusheng@hadoop102 subdir0]$ tar -zxvf tmp.tar.gz</span><br></pre></td></tr></table></figure><p>（4）下载</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[xusheng@hadoop104 software]$ hadoop fs -get &#x2F;jdk-8u212-linux-x64.tar.gz .&#x2F;</span><br></pre></td></tr></table></figure><p>（5）执行 wordcount 程序</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[xusheng@hadoop102 hadoop-3.1.3]$ hadoop jar share&#x2F;hadoop&#x2F;mapreduce&#x2F;hadoop-mapreduce-examples-3.1.3.jar wordcount &#x2F;input &#x2F;output</span><br></pre></td></tr></table></figure><p><img src="/2022/04/11/21104/53.png" alt="在这里插入图片描述"></p><h3 id="4-2-6-配置历史服务器"><a href="#4-2-6-配置历史服务器" class="headerlink" title="4.2.6 配置历史服务器"></a>4.2.6 配置历史服务器</h3><p>为了查看程序的历史运行情况，需要配置一下历史服务器。具体配置步骤如下：<br><strong>1 ） 配置 mapred-site.xml</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[xusheng@hadoop102 hadoop]$ vim mapred-site.xml</span><br></pre></td></tr></table></figure><p>在该文件里面增加如下配置。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&lt;!-- 历史服务器端地址 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;mapreduce.jobhistory.address&lt;&#x2F;name&gt;</span><br><span class="line">    &lt;value&gt;hadoop102:10020&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line">&lt;!-- 历史服务器 web 端地址 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;&#x2F;name&gt;</span><br><span class="line">    &lt;value&gt;hadoop102:19888&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br></pre></td></tr></table></figure><p><strong>2 ） 分发配置</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[xusheng@hadoop102 hadoop]$ xsync $HADOOP_HOME&#x2F;etc&#x2F;hadoop&#x2F;mapred-site.xml</span><br></pre></td></tr></table></figure><p><img src="/2022/04/11/21104/54.png" alt="在这里插入图片描述"></p><p><strong>3 ）在 在 hadoop102 启动历史服务器</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[xusheng@hadoop102 hadoop]$ mapred --daemon start historyserver</span><br></pre></td></tr></table></figure><p><strong>4 ） 查看历史服务器是否启动</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[xusheng@hadoop102 hadoop]$ jps</span><br></pre></td></tr></table></figure><p><img src="/2022/04/11/21104/55.png" alt="在这里插入图片描述"></p><p><strong>5 ）看 查看 JobHistory</strong><br><a href="http://hadoop102:19888/jobhistory" target="_blank" rel="noopener">http://hadoop102:19888/jobhistory</a></p><h3 id="4-2-7-配置日志的聚集"><a href="#4-2-7-配置日志的聚集" class="headerlink" title="4.2.7 配置日志的聚集"></a>4.2.7 配置日志的聚集</h3><p>日志聚集概念：应用运行完成以后，将程序运行日志信息上传到 HDFS 系统上。<br><img src="/2022/04/11/21104/56.png" alt="在这里插入图片描述"><br>日志聚集功能好处：可以方便的查看到程序运行详情，方便开发调试。<br>注意：开启日志聚集功能，需要重新启动 NodeManager 、ResourceManager<br>和HistoryServer。<br>开启日志聚集功能具体步骤如下：<br><strong>1 ）置 配置 yarn-site.xml</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[xusheng@hadoop102 hadoop]$ vim yarn-site.xml</span><br></pre></td></tr></table></figure><p>在该文件里面增加如下配置。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">&lt;!-- 开启日志聚集功能 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.log-aggregation-enable&lt;&#x2F;name&gt;</span><br><span class="line">    &lt;value&gt;true&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line">&lt;!-- 设置日志聚集服务器地址 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.log.server.url&lt;&#x2F;name&gt;</span><br><span class="line">    &lt;value&gt;http:&#x2F;&#x2F;hadoop102:19888&#x2F;jobhistory&#x2F;logs&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line">&lt;!-- 设置日志保留时间为 7 天 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.log-aggregation.retain-seconds&lt;&#x2F;name&gt;</span><br><span class="line">    &lt;value&gt;604800&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br></pre></td></tr></table></figure><p><strong>2 ） 分发配置</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[xusheng@hadoop102 hadoop]$ xsync $HADOOP_HOME&#x2F;etc&#x2F;hadoop&#x2F;yarn-site.xml</span><br></pre></td></tr></table></figure><p><img src="/2022/04/11/21104/57.png" alt="在这里插入图片描述"></p><p><strong>3 ） 关闭 NodeManager 、ResourceManager 和 和 HistoryServer</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[xusheng@hadoop103 hadoop-3.1.3]$ sbin&#x2F;stop-yarn.sh</span><br><span class="line">[xusheng@hadoop103 hadoop-3.1.3]$ mapred --daemon stop historyserver</span><br></pre></td></tr></table></figure><p><img src="/2022/04/11/21104/58.png" alt="在这里插入图片描述"><br><img src="/2022/04/11/21104/59.png" alt="在这里插入图片描述"></p><p><strong>4 ） 启动 NodeManager 、ResourceManage 和 和 HistoryServer</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[xusheng@hadoop103 ~]$ start-yarn.sh</span><br><span class="line">[xusheng@hadoop102 ~]$ mapred --daemon start historyserver</span><br></pre></td></tr></table></figure><p><strong>5 ） 删除 HDFS 上已经存在的 输出 文件</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[xusheng@hadoop102 ~]$ hadoop fs -rm -r &#x2F;output</span><br></pre></td></tr></table></figure><p><strong>6 ） 执行 WordCount 程序</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[xusheng@hadoop102 hadoop-3.1.3]$ hadoop jar</span><br><span class="line">share&#x2F;hadoop&#x2F;mapreduce&#x2F;hadoop-mapreduce-examples-3.1.3.jar</span><br><span class="line">wordcount &#x2F;input &#x2F;output</span><br></pre></td></tr></table></figure><p><strong>7 ） 查看日志</strong><br>（1）历史服务器地址<br><a href="http://hadoop102:19888/jobhistory" target="_blank" rel="noopener">http://hadoop102:19888/jobhistory</a><br>（2）历史任务列表<br><img src="/2022/04/11/21104/60.png" alt="在这里插入图片描述"><br>（3）查看任务运行日志<br><img src="/2022/04/11/21104/61.png" alt="在这里插入图片描述"><br>（4）运行日志详情<br><img src="/2022/04/11/21104/62.png" alt="在这里插入图片描述"></p><h3 id="4-2-8-集群启动-停止方式总结"><a href="#4-2-8-集群启动-停止方式总结" class="headerlink" title="4.2.8 集群启动/停止方式总结"></a>4.2.8 集群启动/停止方式总结</h3><p><strong>1 ） 各个模块分开启动/ 停止 （配置 ssh 是前提） 常用</strong><br>（1）整体启动/停止 HDFS</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">start-dfs.sh&#x2F;stop-dfs.sh</span><br></pre></td></tr></table></figure><p>（2）整体启动/停止 YARN</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">start-yarn.sh&#x2F;stop-yarn.sh</span><br></pre></td></tr></table></figure><p><img src="/2022/04/11/21104/63.png" alt="在这里插入图片描述"></p><p><strong>2 ） 各个服务组件逐一启动/ 停止</strong><br>（1）分别启动/停止 HDFS 组件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs --daemon start&#x2F;stop namenode&#x2F;datanode&#x2F;secondarynamenode</span><br></pre></td></tr></table></figure><p>（2）启动/停止 YARN</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yarn --daemon start&#x2F;stop resourcemanager&#x2F;nodemanager</span><br></pre></td></tr></table></figure><h3 id="4-2-9-编写Hadoop集群常用脚本"><a href="#4-2-9-编写Hadoop集群常用脚本" class="headerlink" title="4.2.9 编写Hadoop集群常用脚本"></a>4.2.9 编写Hadoop集群常用脚本</h3><p><strong>1 ）Hadoop 集群启停脚本（包含 HDFS ，Yarn ，Historyserver<br>）：myhadoop.sh</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[xusheng@hadoop102 ~]$ cd &#x2F;home&#x2F;xusheng&#x2F;bin</span><br><span class="line">[xusheng@hadoop102 bin]$ vim myhadoop.sh</span><br></pre></td></tr></table></figure><p>➢ 输入如下内容</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">#!&#x2F;bin&#x2F;bash</span><br><span class="line"></span><br><span class="line">if [ $# -lt 1 ]</span><br><span class="line">then</span><br><span class="line">    echo &quot;No Args Input...&quot;</span><br><span class="line">    exit ;</span><br><span class="line">fi</span><br><span class="line">case $1 in</span><br><span class="line">&quot;start&quot;)</span><br><span class="line">        echo &quot; &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D; 启动 hadoop 集群&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&quot;</span><br><span class="line">        </span><br><span class="line">        echo &quot; --------------- 启动 hdfs ---------------&quot;</span><br><span class="line">        ssh hadoop102 &quot;&#x2F;opt&#x2F;module&#x2F;hadoop-3.1.3&#x2F;sbin&#x2F;start-dfs.sh&quot;</span><br><span class="line">        echo &quot; --------------- 启动 yarn ---------------&quot;</span><br><span class="line">        ssh hadoop103 &quot;&#x2F;opt&#x2F;module&#x2F;hadoop-3.1.3&#x2F;sbin&#x2F;start-yarn.sh&quot;</span><br><span class="line">        echo &quot; --------------- 启动 historyserver ---------------&quot;</span><br><span class="line">        ssh hadoop102 &quot;&#x2F;opt&#x2F;module&#x2F;hadoop-3.1.3&#x2F;bin&#x2F;mapred --daemon start historyserver&quot;</span><br><span class="line">;;</span><br><span class="line">&quot;stop&quot;)</span><br><span class="line">        echo &quot; &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D; 关闭 hadoop 集群 &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&quot;</span><br><span class="line">        echo &quot; --------------- 关闭 historyserver ---------------&quot;</span><br><span class="line">        ssh hadoop102 &quot;&#x2F;opt&#x2F;module&#x2F;hadoop-3.1.3&#x2F;bin&#x2F;mapred --daemon stop historyserver&quot;</span><br><span class="line">        echo &quot; --------------- 关闭 yarn ---------------&quot;</span><br><span class="line">        ssh hadoop103 &quot;&#x2F;opt&#x2F;module&#x2F;hadoop-3.1.3&#x2F;sbin&#x2F;stop-yarn.sh&quot;</span><br><span class="line">        echo &quot; --------------- 关闭 hdfs ---------------&quot;</span><br><span class="line">        ssh hadoop102 &quot;&#x2F;opt&#x2F;module&#x2F;hadoop-3.1.3&#x2F;sbin&#x2F;stop-dfs.sh&quot;</span><br><span class="line">;;</span><br><span class="line">*)</span><br><span class="line">    echo &quot;Input Args Error...&quot;</span><br><span class="line">;;</span><br><span class="line">esac</span><br></pre></td></tr></table></figure><p><img src="/2022/04/11/21104/64.png" alt="在这里插入图片描述"></p><p>➢ 保存后退出，然后赋予脚本执行权限</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[xusheng@hadoop102 bin]$ chmod +x myhadoop.sh</span><br></pre></td></tr></table></figure><p><img src="/2022/04/11/21104/65.png" alt="在这里插入图片描述"><br><img src="/2022/04/11/21104/66.png" alt="在这里插入图片描述"></p><p><strong>2 ）查看三台服务器 Java 进程脚本：jpsall</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[xusheng@hadoop102 ~]$ cd &#x2F;home&#x2F;xusheng&#x2F;bin</span><br><span class="line">[xusheng@hadoop102 bin]$ vim jpsall</span><br></pre></td></tr></table></figure><p>➢ 输入如下内容</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">#!&#x2F;bin&#x2F;bash</span><br><span class="line"></span><br><span class="line">for host in hadoop102 hadoop103 hadoop104</span><br><span class="line">do</span><br><span class="line">        echo &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D; $host &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line">        ssh $host jps</span><br><span class="line">done</span><br></pre></td></tr></table></figure><p>➢ 保存后退出，然后赋予脚本执行权限</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[xusheng@hadoop102 bin]$ chmod +x jpsall</span><br></pre></td></tr></table></figure><p><img src="/2022/04/11/21104/67.png" alt="在这里插入图片描述"></p><p><strong>3 ）分发/home/xusheng/bin 目录，保证自定义脚本在三台机器上都可以使用</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[xusheng@hadoop102 ~]$ xsync &#x2F;home&#x2F;xusheng&#x2F;bin&#x2F;</span><br></pre></td></tr></table></figure><h3 id="4-2-10-常用端口号说明"><a href="#4-2-10-常用端口号说明" class="headerlink" title="4.2.10 常用端口号说明"></a>4.2.10 常用端口号说明</h3><p><img src="/2022/04/11/21104/68.png" alt="在这里插入图片描述"></p><h3 id="4-2-11-集群时间同步"><a href="#4-2-11-集群时间同步" class="headerlink" title="4.2.11 集群时间同步"></a>4.2.11 集群时间同步</h3><p>如果服务器在公网环境（能连接外网），可以不采用集群时间同步，因为服务器会定期和公网时间进行校准；<br>如果服务器在内网环境，必须要配置集群时间同步，否则时间久了，会产生时间偏差，导致集群执行任务时间不同步。<br><strong>1 ）需求</strong><br>找一个机器，作为时间服务器，所有的机器与这台集群时间进行定时的同步，生产环境根据任务对时间的准确程度要求周期同步。测试环境为了尽快看到效果，采用<br>1 分钟同步一次。<br><img src="/2022/04/11/21104/69.png" alt="在这里插入图片描述"><br><strong>2 ） 时间服务器配置（必须 root 用户）</strong><br>（1）查看所有节点 ntpd 服务状态和开机自启动状态</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[xusheng@hadoop102 ~]$ sudo systemctl status ntpd</span><br><span class="line">[xusheng@hadoop102 ~]$ sudo systemctl start ntpd</span><br><span class="line">[xusheng@hadoop102 ~]$ sudo systemctl is-enabled ntpd</span><br></pre></td></tr></table></figure><p>（2）修改 hadoop102 的 ntp.conf 配置文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[xusheng@hadoop102 ~]$ sudo vim &#x2F;etc&#x2F;ntp.conf</span><br></pre></td></tr></table></figure><p>修改内容如下<br>（a）修改 1（授权 192.168.10.0-192.168.10.255<br>网段上的所有机器可以从这台机器上查询和同步时间）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">#restrict 192.168.10.0 mask 255.255.255.0 nomodify notrap</span><br></pre></td></tr></table></figure><p>为</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">restrict 192.168.10.0 mask 255.255.255.0 nomodify notrap</span><br></pre></td></tr></table></figure><p>（b）修改 2（集群在局域网中，不使用其他互联网上的时间）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">server 0.centos.pool.ntp.org iburst</span><br><span class="line">server 1.centos.pool.ntp.org iburst</span><br><span class="line">server 2.centos.pool.ntp.org iburst</span><br><span class="line">server 3.centos.pool.ntp.org iburst</span><br></pre></td></tr></table></figure><p>为</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">#server 0.centos.pool.ntp.org iburst</span><br><span class="line">#server 1.centos.pool.ntp.org iburst</span><br><span class="line">#server 2.centos.pool.ntp.org iburst</span><br><span class="line">#server 3.centos.pool.ntp.org iburst</span><br></pre></td></tr></table></figure><p>（c）添加 3 （<br>当该节点丢失网络连接，依然可以采用本地时间作为时间服务器为集群中<br>的其他节点提供时间同步 ）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">server 127.127.1.0</span><br><span class="line">fudge 127.127.1.0 stratum 10</span><br></pre></td></tr></table></figure><p>（3）修改 hadoop102 的/etc/sysconfig/ntpd 文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[xusheng@hadoop102 ~]$ sudo vim &#x2F;etc&#x2F;sysconfig&#x2F;ntpd</span><br></pre></td></tr></table></figure><p>增加内容如下（让硬件时间与系统时间一起同步）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SYNC_HWCLOCK&#x3D;yes</span><br></pre></td></tr></table></figure><p>（4）重新启动 ntpd 服务</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[xusheng@hadoop102 ~]$ sudo systemctl start ntpd</span><br></pre></td></tr></table></figure><p>（5）设置 ntpd 服务开机启动</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[xusheng@hadoop102 ~]$ sudo systemctl enable ntpd</span><br></pre></td></tr></table></figure><p><strong>3 ） 其他机器配置（必须 root 用户）</strong><br>（1）关闭所有节点上 ntp 服务和自启动</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[xusheng@hadoop103 ~]$ sudo systemctl stop ntpd</span><br><span class="line">[xusheng@hadoop103 ~]$ sudo systemctl disable ntpd</span><br><span class="line">[xusheng@hadoop104 ~]$ sudo systemctl stop ntpd</span><br><span class="line">[xusheng@hadoop104 ~]$ sudo systemctl disable ntpd</span><br></pre></td></tr></table></figure><p>（2）在其他机器配置 1 分钟与时间服务器同步一次</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[xusheng@hadoop103 ~]$ sudo crontab -e</span><br></pre></td></tr></table></figure><p>编写定时任务如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">*&#x2F;1 * * * * &#x2F;usr&#x2F;sbin&#x2F;ntpdate hadoop102</span><br></pre></td></tr></table></figure><p>（3）修改任意机器时间</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[xusheng@hadoop103 ~]$ sudo date -s &quot;2021-9-11 11:11:11&quot;</span><br></pre></td></tr></table></figure><p>（4）1 分钟后查看机器是否与时间服务器同步</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[xusheng@hadoop103 ~]$ sudo date</span><br></pre></td></tr></table></figure><h1 id="五、常见错误及解决方案"><a href="#五、常见错误及解决方案" class="headerlink" title="五、常见错误及解决方案"></a>五、常见错误及解决方案</h1><p>1）防火墙没关闭、或者没有启动 YARN</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">INFO client.RMProxy: Connecting to ResourceManager at hadoop108&#x2F;192.168.10.108:8032</span><br></pre></td></tr></table></figure><p>2）主机名称配置错误<br>3）IP 地址配置错误<br>4）ssh 没有配置好<br>5）root 用户和 xusheng两个用户启动集群不统一<br>6）配置文件修改不细心<br>7）不识别主机名称</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">java.net.UnknownHostException: hadoop102: hadoop102</span><br><span class="line">at</span><br><span class="line">java.net.InetAddress.getLocalHost(InetAddress.java:1475)</span><br><span class="line">at</span><br><span class="line">org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(Job</span><br><span class="line">Submitter.java:146)</span><br><span class="line">at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1290)</span><br><span class="line">at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1287)</span><br><span class="line">at java.security.AccessController.doPrivileged(Native</span><br><span class="line">Method)</span><br><span class="line">at javax.security.auth.Subject.doAs(Subject.java:415)</span><br></pre></td></tr></table></figure><p>解决办法：<br>（1）在/etc/hosts 文件中添加 192.168.10.102 hadoop102<br>（2）主机名称不要起 hadoop hadoop000 等特殊名称<br>8）DataNode 和 NameNode 进程同时只能工作一个。<br><img src="/2022/04/11/21104/70.png" alt="在这里插入图片描述"><br>9）执行命令不生效，粘贴 Word中命令时，遇到-和长–没区分开。导致命令失效解决办法：尽量不要粘贴 Word 中代码。<br>10）jps 发现进程已经没有，但是重新启动集群，提示进程已经开启。原因是在 Linux 的根目录下/tmp目录中存在启动的进程临时文件，将集群相关进程删除掉，再重新启动集群。<br>11）jps 不生效原因：全局变量 hadoop java 没有生效。解决办法：需要 source /etc/profile文件。<br>12）8088 端口连接不上</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[xusheng@hadoop102 桌面]$ cat &#x2F;etc&#x2F;hosts</span><br></pre></td></tr></table></figure><p>注释掉如下代码：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">#127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4</span><br><span class="line">#::1 hadoop102</span><br></pre></td></tr></table></figure><p>13) 启动hdfs时遇到如下报错</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">ERROR: Attempting to operate on hdfs namenode as root</span><br><span class="line">ERROR: but there is no HDFS_NAMENODE_USER defined. Aborting operation.</span><br><span class="line">Starting datanodes</span><br><span class="line">ERROR: Attempting to operate on hdfs datanode as root</span><br><span class="line">ERROR: but there is no HDFS_DATANODE_USER defined. Aborting operation.</span><br><span class="line">Starting secondary namenodes [node03]</span><br><span class="line">ERROR: Attempting to operate on hdfs secondarynamenode as root</span><br><span class="line">ERROR: but there is no HDFS_SECONDARYNAMENODE_USER defined. Aborting operation.</span><br></pre></td></tr></table></figure><p>猜想应该是用root用户启动的，ssh免登陆做的也是root用户</p><blockquote><p>解决</p><p>对于start-dfs.sh和stop-dfs.sh文件，添加下列参数：</p><p>HDFS_DATANODE_USER=root<br>HADOOP_SECURE_DN_USER=hdfs<br>HDFS_NAMENODE_USER=root<br>HDFS_SECONDARYNAMENODE_USER=root<br>对于start-yarn.sh和stop-yarn.sh文件，添加下列参数：</p><p>YARN_RESOURCEMANAGER_USER=root<br>HADOOP_SECURE_DN_USER=yarn<br>YARN_NODEMANAGER_USER=root</p></blockquote><p>14) JAVA_HOME找不到错误，但本机JAVA_HOME配置正确，直接修改$HADOOP_HOME/etc/hadoop/hdoop-env.sh</p><p>放开export JAVA_HOME=$JAVA_HOME注释，替换成真实JAVA_HOME路径</p><p>15) 完全分布式运行模式，执行wordcount示例程序卡在<code>INFO mapreduce.Job: Running job</code></p><p>在yarn-site.xml中加</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.nodemanager.resource.memory-mb&lt;&#x2F;name&gt;</span><br><span class="line">    &lt;value&gt;2048&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br></pre></td></tr></table></figure><p>文章转自<a href="https://blog.csdn.net/m0_52435951/article/details/123986924?utm_medium=distribute.pc_relevant.none-task-blog-2~default~baidujs_title~default-5.pc_relevant_default&amp;spm=1001.2101.3001.4242.4&amp;utm_relevant_index=8，学习用，如有侵权，联系删" target="_blank" rel="noopener">https://blog.csdn.net/m0_52435951/article/details/123986924?utm_medium=distribute.pc_relevant.none-task-blog-2~default~baidujs_title~default-5.pc_relevant_default&amp;spm=1001.2101.3001.4242.4&amp;utm_relevant_index=8，学习用，如有侵权，联系删</a></p></div><div><ul class="post-copyright"><li class="post-copyright-author"><strong>Post author:</strong> 流水</li><li class="post-copyright-link"><strong>Post link:</strong> <a href="https://yjdoudou.github.io/2022/04/11/21104/" title="hadoop安装">https://yjdoudou.github.io/2022/04/11/21104/</a></li><li class="post-copyright-license"><strong>Copyright Notice: </strong>All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/3.0/" rel="external nofollow" target="_blank">CC BY-NC-SA 3.0</a> unless stating additionally.</li></ul></div><footer class="post-footer"><div class="post-tags"><a href="/tags/hadoop%EF%BC%8C%E5%A4%A7%E6%95%B0%E6%8D%AE/" rel="tag"># hadoop，大数据</a></div><div class="post-nav"><div class="post-nav-next post-nav-item"><a href="/2022/02/16/23212/" rel="next" title="log4j渗透solr"><i class="fa fa-chevron-left"></i> log4j渗透solr</a></div><span class="post-nav-divider"></span><div class="post-nav-prev post-nav-item"></div></div></footer></div></article><div class="post-spread"></div></div></div><div class="comments" id="comments"></div></div><div class="sidebar-toggle"><div class="sidebar-toggle-line-wrap"><span class="sidebar-toggle-line sidebar-toggle-line-first"></span> <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span> <span class="sidebar-toggle-line sidebar-toggle-line-last"></span></div></div><aside id="sidebar" class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">Table of Contents</li><li class="sidebar-nav-overview" data-target="site-overview-wrap">Overview</li></ul><section class="site-overview-wrap sidebar-panel"><div class="site-overview"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" src="/images/head.jpg" alt="流水"><p class="site-author-name" itemprop="name">流水</p><p class="site-description motion-element" itemprop="description">衣带渐宽终不悔<br>为伊消得人憔悴</p></div><nav class="site-state motion-element"><div class="site-state-item site-state-posts"><a href="/archives"><span class="site-state-item-count">22</span> <span class="site-state-item-name">posts</span></a></div><div class="site-state-item site-state-categories"><a href="/categories/index.html"><span class="site-state-item-count">13</span> <span class="site-state-item-name">categories</span></a></div><div class="site-state-item site-state-tags"><a href="/tags/index.html"><span class="site-state-item-count">18</span> <span class="site-state-item-name">tags</span></a></div></nav><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://www.cnblogs.com/yjdoudou" target="_blank" title="博客园"><i class="fa fa-fw fa-github"></i>博客园</a> </span><span class="links-of-author-item"><a href="mailto:1769072244@qq.com" target="_blank" title="E-Mail"><i class="fa fa-fw fa-envelope"></i>E-Mail</a></span></div></div></section><section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active"><div class="post-toc"><div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#一、大数据概论"><span class="nav-number">1.</span> <span class="nav-text">一、大数据概论</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#二、Hadoop概述"><span class="nav-number">2.</span> <span class="nav-text">二、Hadoop概述</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#三、Hadoop-运行环境搭建-（开发-重点）"><span class="nav-number">3.</span> <span class="nav-text">三、Hadoop 运行环境搭建 （开发 重点）</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#3-1-模板虚拟机环境准备"><span class="nav-number">3.1.</span> <span class="nav-text">3.1 模板虚拟机环境准备</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-2-克隆虚拟机"><span class="nav-number">3.2.</span> <span class="nav-text">3.2 克隆虚拟机</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-3-在hadoop102安装JDK"><span class="nav-number">3.3.</span> <span class="nav-text">3.3 在hadoop102安装JDK</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-4-在hadoop102安装Hadoop"><span class="nav-number">3.4.</span> <span class="nav-text">3.4 在hadoop102安装Hadoop</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-5-Hadoop目录结构"><span class="nav-number">3.5.</span> <span class="nav-text">3.5 Hadoop目录结构</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#四、Hadoop运行模式"><span class="nav-number">4.</span> <span class="nav-text">四、Hadoop运行模式</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#4-1-本地运行模式（官方WordCount）"><span class="nav-number">4.1.</span> <span class="nav-text">4.1 本地运行模式（官方WordCount）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-2-完全分布式运行模式（开发重点）"><span class="nav-number">4.2.</span> <span class="nav-text">4.2 完全分布式运行模式（开发重点）</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#4-2-1-虚拟机准备"><span class="nav-number">4.2.1.</span> <span class="nav-text">4.2.1 虚拟机准备</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-2-2-编写集群分发脚本xsync"><span class="nav-number">4.2.2.</span> <span class="nav-text">4.2.2 编写集群分发脚本xsync</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-2-3-SSH无密登录配置"><span class="nav-number">4.2.3.</span> <span class="nav-text">4.2.3 SSH无密登录配置</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-2-4-集群配置"><span class="nav-number">4.2.4.</span> <span class="nav-text">4.2.4 集群配置</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-2-5-群起集群"><span class="nav-number">4.2.5.</span> <span class="nav-text">4.2.5 群起集群</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-2-6-配置历史服务器"><span class="nav-number">4.2.6.</span> <span class="nav-text">4.2.6 配置历史服务器</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-2-7-配置日志的聚集"><span class="nav-number">4.2.7.</span> <span class="nav-text">4.2.7 配置日志的聚集</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-2-8-集群启动-停止方式总结"><span class="nav-number">4.2.8.</span> <span class="nav-text">4.2.8 集群启动&#x2F;停止方式总结</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-2-9-编写Hadoop集群常用脚本"><span class="nav-number">4.2.9.</span> <span class="nav-text">4.2.9 编写Hadoop集群常用脚本</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-2-10-常用端口号说明"><span class="nav-number">4.2.10.</span> <span class="nav-text">4.2.10 常用端口号说明</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-2-11-集群时间同步"><span class="nav-number">4.2.11.</span> <span class="nav-text">4.2.11 集群时间同步</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#五、常见错误及解决方案"><span class="nav-number">5.</span> <span class="nav-text">五、常见错误及解决方案</span></a></li></ol></div></div></section><div class="back-to-top"><i class="fa fa-arrow-up"></i> <span id="scrollpercent"><span>0</span>%</span></div></div></aside></div></main><footer id="footer" class="footer"><div class="footer-inner"><div class="copyright">&copy; 2020 &mdash; <span itemprop="copyrightYear">2022</span> <span class="with-love"><i class="fa fa-[object Object]"></i> </span><span class="author" itemprop="copyrightHolder">流水</span></div><div class="busuanzi-count"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span class="site-uv">你是来访的第 <span class="busuanzi-value" id="busuanzi_value_site_uv"></span> 位小伙伴</span></div></div></footer></div><script type="text/javascript">"[object Function]"!==Object.prototype.toString.call(window.Promise)&&(window.Promise=null)</script><script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script><script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script><script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script><script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script><script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script><script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script><script src="//unpkg.com/valine/dist/Valine.min.js"></script><script type="text/javascript">var GUEST = ['nick','mail','link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item=>{
      return GUEST.indexOf(item)>-1;
    });
    new Valine({
        el: '#comments' ,
        verify: true,
        notify: true,
        appId: 'M9ucAtVAq5dTwQEePYAFMiNm-gzGzoHsz',
        appKey: 'tOVH9XTcixGijCwoxcES18EN',
        placeholder: 'Just go go',
        avatar:'mm',
        guest_info:guest,
        pageSize:'10' || 10,
    });</script><script type="text/javascript">// Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });</script></body></html><!-- rebuild by neat -->